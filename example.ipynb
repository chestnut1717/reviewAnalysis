{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53f90335-ca8f-4dc1-b202-61cfe749946e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. 모듈로 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40c2ce53-23e9-4d10-84da-daa09bbb71f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scraper.blogscraper import BlogScraper\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d44f3227-8022-4b3a-9790-38211fc2fff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_30068\\3100810723.py:3: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(executable_path='driver/chromedriver')\n"
     ]
    }
   ],
   "source": [
    "# 크롬 드라이버 로드\n",
    "## 각자 버전에 맞는 드라이버 다운 후, chromedriver.exe파일의 경로 입력\n",
    "driver = webdriver.Chrome(executable_path='driver/chromedriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1fd444b-2970-4311-abd8-be4f028ffc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:05<00:00,  2.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "블로그 url 수집 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:10<00:00,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "작업 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sc = BlogScraper(driver = driver, \n",
    "                 keyword='gs25 뭘좋아할지몰라다넣어봤어 도시락',\n",
    "                 page_range=2,\n",
    "                 start_date='2020-07-15',\n",
    "                 end_date='2022-07-17')\n",
    "\n",
    "sc.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7774e1f6-b01c-4147-9e4d-c1bdfd3528b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# excel 파일 저장(파일이름)\n",
    "sc.save_excel('data/example.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0e86cd0-1875-4034-be76-d829f7b0c069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv파일 저장\n",
    "sc.save_csv('data/example.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f51d750-0bbc-4e98-9710-cddd14318534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraper 종료코드\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c3c925-8245-4331-99d6-eb40841e8974",
   "metadata": {},
   "source": [
    "## 2. 코드 복사해서 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c9b4ead-5bc7-44a1-af57-fc70fadefbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# precess bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "class BlogScraper():\n",
    "    def __init__(self, driver, keyword, page_range, start_date, end_date):\n",
    "        self.keyword    = keyword\n",
    "        self.start_date = start_date\n",
    "        self.end_date   = end_date\n",
    "        self.driver     = driver\n",
    "        self.datafile   = pd.DataFrame(columns=['Title', 'Text', 'URL'])\n",
    "        \n",
    "        if type(page_range) != int or page_range <= 0:\n",
    "            raise Exception('올바른 페이지 형식이 아닙니다')\n",
    "        else:\n",
    "            self.page_range = page_range\n",
    "            \n",
    "    \n",
    "    # 블로그 page의 url 가져오기\n",
    "    def get_urls(self, page):\n",
    "        url = f\"https://section.blog.naver.com/Search/Post.naver?pageNo={page}&rangeType=PERIOD&orderBy=sim&startDate={self.start_date}&endDate={self.end_date}&keyword={self.keyword}\"\n",
    "\n",
    "        return url\n",
    "    \n",
    "    # 블로그 스크레이핑\n",
    "    def text_scraping(self, url):\n",
    "        headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36\"}\n",
    "        res = requests.get(url, headers=headers)\n",
    "        res.raise_for_status() # 문제시 프로그램 종료\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\") \n",
    "\n",
    "        if soup.find(\"div\", attrs={\"class\":\"se-main-container\"}):\n",
    "            text = soup.find(\"div\", attrs={\"class\":\"se-main-container\"}).get_text()\n",
    "            text = text.replace(\"\\n\",\"\") #공백 제거\n",
    "            \n",
    "            # title을 확인 못할때 예외처리\n",
    "            try:\n",
    "                title = soup.select('.se-fs-')[0].text\n",
    "            except:\n",
    "                return text, None\n",
    "                pass\n",
    "            return text, title\n",
    "        else:\n",
    "            return \"확인불가\"\n",
    "        \n",
    "    # iframe 제거\n",
    "    ## naver blog의 iframe의 요소때문에 정상적으로 글이 크롤링되지 않는데, 이것을 통해서 새로운 url 가져온다\n",
    "    def delete_iframe(self, url):\n",
    "        headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36\"}\n",
    "        res = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\") \n",
    "\n",
    "        src_url = \"https://blog.naver.com/\" + soup.iframe[\"src\"]\n",
    "\n",
    "        return src_url\n",
    "    \n",
    "    \n",
    "    def extract_url(self):\n",
    "        # url이 저장될 list\n",
    "        url_list = []\n",
    "        if self.page_range == False:\n",
    "            end_page = 500\n",
    "        else:\n",
    "            end_page = self.page_range + 1\n",
    "            \n",
    "        for page in tqdm(range(1, end_page)):\n",
    "                # 블로그 페이지 url 가져오기\n",
    "                source_url = self.get_urls(page)\n",
    "\n",
    "                # page source 가져오기\n",
    "                self.driver.get(url=source_url)\n",
    "                \n",
    "                # driver에서 실제로 webpage 보여주려면 일정 시간 지나야하기 때문에 일부러 sleep 걸어준다\n",
    "                time.sleep(2)\n",
    "                \n",
    "                html = self.driver.page_source\n",
    "                soup = BeautifulSoup(html, \"html.parser\")\n",
    "                \n",
    "                # 블로그 페이지에서 각 블로그 url추출\n",
    "                try:\n",
    "                    for i in range(1, 8):\n",
    "                        url = soup.select(f\"#content > section > div.area_list_search > div:nth-child({i}) > div > div.info_post > div.desc > a.text\")[0]['ng-href']\n",
    "                        url_list.append(url)\n",
    "                # 모든 페이지를 탐색했으면 종료        \n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    break\n",
    "        \n",
    "        print('블로그 url 수집 완료')\n",
    "        return url_list\n",
    "    \n",
    "    # pandas의 dataframe파일로 저장한다\n",
    "    def save_datafile(self, url_list):\n",
    "        for url in tqdm(url_list):\n",
    "            re_url = self.delete_iframe(url)\n",
    "            \n",
    "            data = self.text_scraping(re_url)\n",
    "            \n",
    "            # title이 추출되지 못한 경우\n",
    "            if len(data) == 2:\n",
    "                text, title = data[0], data[1]\n",
    "            else:\n",
    "                text, title = data, None\n",
    "                \n",
    "            tmp = pd.DataFrame({'Title' : [title] , 'Text' : [text], 'URL' : [re_url]})\n",
    "            self.datafile = pd.concat([self.datafile, tmp])\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        print('작업 완료')\n",
    "     \n",
    "    def run(self):\n",
    "        url_list = self.extract_url()\n",
    "        self.save_datafile(url_list)\n",
    "        \n",
    "    \n",
    "    def save_csv(self, filename):\n",
    "        self.datafile.to_csv(f'{filename}', index=False)\n",
    "    \n",
    "    def save_excel(self, filename):\n",
    "        self.datafile.to_excel(f'{filename}', index=False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad3e1a86-ff0a-4086-b76d-69343e1f8394",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_22596\\2237476259.py:5: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(executable_path='driver/chromedriver')\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "# 크롬 드라이버 로드\n",
    "## 각자 버전에 맞는 드라이버 다운 후, chromedriver.exe파일의 경로 입력\n",
    "driver = webdriver.Chrome(executable_path='driver/chromedriver')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3eeb8e1-7a01-4a32-97a7-004ee4a4e072",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:05<00:00,  2.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "블로그 url 수집 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:10<00:00,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "작업 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sc = BlogScraper(driver = driver, \n",
    "                 keyword='gs25 뭘좋아할지몰라다넣어봤어 도시락',\n",
    "                 page_range=2,\n",
    "                 start_date='2020-07-15',\n",
    "                 end_date='2022-07-17')\n",
    "\n",
    "sc.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ad4ba64-9639-49b2-963e-86157b652bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# excel 파일 저장(파일이름)\n",
    "sc.save_excel('data/example.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab22ee3f-9130-4890-8d5b-5cfc1a6afe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv파일 저장\n",
    "sc.save_csv('data/example.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "079fbc44-cbd6-469f-88ae-1cd6e46d2e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraper 종료코드\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b351f01-2c23-4469-9c5b-c6c0a6a6a362",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
