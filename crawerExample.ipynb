{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "848ba89c-4a14-4c52-ba55-ea2017a9a73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\anaconda\\envs\\nlp\\lib\\site-packages (4.64.0)\n",
      "Requirement already satisfied: colorama in c:\\anaconda\\envs\\nlp\\lib\\site-packages (from tqdm) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "# # !pip install bs4\n",
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29fba644-de43-4258-8531-d081cee8a23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "# precess bar\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082f4618-9f6c-4f3a-bfc3-006beaa1072a",
   "metadata": {},
   "source": [
    "## 1. 네이버 블로그 크롤링 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4ccc471-383b-4e58-83b2-6d1ed5d496b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlogScraper():\n",
    "    def __init__(self, keyword, page_range, start_date, end_date):\n",
    "        self.keyword    = keyword\n",
    "        self.start_date = start_date\n",
    "        self.end_date   = end_date\n",
    "        self.driver     = webdriver.Chrome(executable_path='chromedriver')\n",
    "        self.datafile   = pd.DataFrame(columns=['Title', 'Text', 'URL'])\n",
    "        \n",
    "        if type(page_range) != int or page_range <= 0:\n",
    "            raise Exception('올바른 페이지 형식이 아닙니다')\n",
    "        else:\n",
    "            self.page_range = page_range\n",
    "            \n",
    "    \n",
    "    # 블로그 page의 url 가져오기\n",
    "    def get_urls(self, page):\n",
    "        url = f\"https://section.blog.naver.com/Search/Post.naver?pageNo={page}&rangeType=PERIOD&orderBy=sim&startDate={self.start_date}&endDate={self.end_date}&keyword={self.keyword}\"\n",
    "\n",
    "        return url\n",
    "    \n",
    "    # 블로그 스크레이핑\n",
    "    def text_scraping(self, url):\n",
    "        headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36\"}\n",
    "        res = requests.get(url, headers=headers)\n",
    "        res.raise_for_status() # 문제시 프로그램 종료\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\") \n",
    "\n",
    "        if soup.find(\"div\", attrs={\"class\":\"se-main-container\"}):\n",
    "            text = soup.find(\"div\", attrs={\"class\":\"se-main-container\"}).get_text()\n",
    "            text = text.replace(\"\\n\",\"\") #공백 제거\n",
    "            \n",
    "            # title을 확인 못할때 예외처리\n",
    "            try:\n",
    "                title = soup.select('.se-fs-')[0].text\n",
    "            except:\n",
    "                return text, None\n",
    "                pass\n",
    "            return text, title\n",
    "        else:\n",
    "            return \"확인불가\"\n",
    "        \n",
    "    # iframe 제거\n",
    "    ## naver blog의 iframe의 요소때문에 정상적으로 글이 크롤링되지 않는데, 이것을 통해서 새로운 url 가져온다\n",
    "    def delete_iframe(self, url):\n",
    "        headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36\"}\n",
    "        res = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\") \n",
    "\n",
    "        src_url = \"https://blog.naver.com/\" + soup.iframe[\"src\"]\n",
    "\n",
    "        return src_url\n",
    "    \n",
    "    \n",
    "    def extract_url(self):\n",
    "        # url이 저장될 list\n",
    "        url_list = []\n",
    "        if self.page_range == False:\n",
    "            end_page = 500\n",
    "        else:\n",
    "            end_page = self.page_range + 1\n",
    "            \n",
    "        for page in tqdm(range(1, end_page)):\n",
    "                # 블로그 페이지 url 가져오기\n",
    "                source_url = self.get_urls(page)\n",
    "\n",
    "                # page source 가져오기\n",
    "                self.driver.get(url=source_url)\n",
    "                \n",
    "                # driver에서 실제로 webpage 보여주려면 일정 시간 지나야하기 때문에 일부러 sleep 걸어준다\n",
    "                time.sleep(2)\n",
    "                \n",
    "                html = self.driver.page_source\n",
    "                soup = BeautifulSoup(html, \"html.parser\")\n",
    "                \n",
    "                # 블로그 페이지에서 각 블로그 url추출\n",
    "                try:\n",
    "                    for i in range(1, 8):\n",
    "                        url = soup.select(f\"#content > section > div.area_list_search > div:nth-child({i}) > div > div.info_post > div.desc > a.text\")[0]['ng-href']\n",
    "                        url_list.append(url)\n",
    "                # 모든 페이지를 탐색했으면 종료        \n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    break\n",
    "        \n",
    "        print('블로그 url 수집 완료')\n",
    "        return url_list\n",
    "    \n",
    "    # pandas의 dataframe파일로 저장한다\n",
    "    def save_datafile(self, url_list):\n",
    "        for url in tqdm(url_list):\n",
    "            re_url = self.delete_iframe(url)\n",
    "            \n",
    "            data = self.text_scraping(re_url)\n",
    "            \n",
    "            # title이 추출되지 못한 경우\n",
    "            if len(data) == 2:\n",
    "                text, title = data[0], data[1]\n",
    "            else:\n",
    "                text, title = data, None\n",
    "                \n",
    "            tmp = pd.DataFrame({'Title' : [title] , 'Text' : [text], 'URL' : [re_url]})\n",
    "            self.datafile = pd.concat([self.datafile, tmp])\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        print('작업 완료')\n",
    "     \n",
    "    def run(self):\n",
    "        url_list = self.extract_url()\n",
    "        self.save_datafile(url_list)\n",
    "        \n",
    "    \n",
    "    def save_csv(self, filename):\n",
    "        self.datafile.to_csv(f'{filename}.csv', index=False)\n",
    "    \n",
    "    def save_excel(self, filename):\n",
    "        self.datafile.to_excel(f'{filename}.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3d3c458-cdc4-4574-9699-b96b5c99a277",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:12<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "블로그 url 수집 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:25<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "작업 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sc = BlogScraper(keyword='gs25 뭘좋아할지몰라다넣어봤어 도시락', page_range=5, start_date='2020-07-15', end_date='2022-07-17')\n",
    "sc.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55162e57-c87b-4f9d-a7a9-849a6738bc09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sc.datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a9f55f8-f8ea-4d8f-9985-1c31bdd76fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv파일 저장\n",
    "sc.save_csv('example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "082ab665-7563-4029-ae2f-0e06bba9f419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# excel 파일 저장\n",
    "sc.save_excel('example')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
